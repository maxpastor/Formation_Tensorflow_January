{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/max/.local/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:107: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Training step: 0000 cost= 2.305939198 Accuray:  inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:65: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step: 0050 cost= 2.292232513 Accuray:  0.09099999964237213\n",
      "Training step: 0100 cost= 2.282411098 Accuray:  0.10189999982714654\n",
      "Training step: 0150 cost= 2.265269279 Accuray:  0.11146666673322518\n",
      "Training step: 0200 cost= 2.252651930 Accuray:  0.12225000014528632\n",
      "Training step: 0250 cost= 2.231728315 Accuray:  0.13196000017225742\n",
      "Training step: 0300 cost= 2.222627878 Accuray:  0.1439666667704781\n",
      "Training step: 0350 cost= 2.190390110 Accuray:  0.15948571448879584\n",
      "Training step: 0400 cost= 2.174775600 Accuray:  0.17515000031329692\n",
      "Training step: 0450 cost= 2.174567223 Accuray:  0.19215555599166287\n",
      "Training step: 0500 cost= 2.155566692 Accuray:  0.20868000034242867\n",
      "Training step: 0550 cost= 2.177771091 Accuray:  0.227163636718284\n",
      "Training step: 0600 cost= 2.112374306 Accuray:  0.2467333335491518\n",
      "Training step: 0650 cost= 2.082293749 Accuray:  0.26718461525554843\n",
      "Training step: 0700 cost= 2.117169857 Accuray:  0.28612857080463855\n",
      "Training step: 0750 cost= 2.050068855 Accuray:  0.30522666591902575\n",
      "Training step: 0800 cost= 2.049501896 Accuray:  0.32179999897722156\n",
      "Training step: 0850 cost= 2.013162136 Accuray:  0.33904705800116064\n",
      "Training step: 0900 cost= 2.002973557 Accuray:  0.3546444437445866\n",
      "Training step: 0950 cost= 2.003090620 Accuray:  0.3697052625174585\n",
      "Training step: 1000 cost= 1.938338757 Accuray:  0.38408999939635396\n",
      "Training step: 1050 cost= 1.924970984 Accuray:  0.3974095232323522\n",
      "Training step: 1100 cost= 1.860810995 Accuray:  0.40934545394033195\n",
      "Training step: 1150 cost= 1.833881378 Accuray:  0.42122608647722265\n",
      "Training step: 1200 cost= 1.823726177 Accuray:  0.43302499981286624\n",
      "Training step: 1250 cost= 1.736267567 Accuray:  0.4437839998871088\n",
      "Training step: 1300 cost= 1.727431297 Accuray:  0.4543153843197685\n",
      "Training step: 1350 cost= 1.576720476 Accuray:  0.46445185163782704\n",
      "Training step: 1400 cost= 1.615202427 Accuray:  0.47369285687005946\n",
      "Training step: 1450 cost= 1.518312097 Accuray:  0.4825517238959156\n",
      "Training step: 1500 cost= 1.566356301 Accuray:  0.4911199998979767\n",
      "Training step: 1550 cost= 1.502679706 Accuray:  0.49939354832374283\n",
      "Training step: 1600 cost= 1.443048716 Accuray:  0.507068749901373\n",
      "Training step: 1650 cost= 1.449345708 Accuray:  0.5147515150385372\n",
      "Training step: 1700 cost= 1.329791069 Accuray:  0.521982352814692\n",
      "Training step: 1750 cost= 1.330108643 Accuray:  0.5290285712416682\n",
      "Training step: 1800 cost= 1.271373034 Accuray:  0.536022222019318\n",
      "Training step: 1850 cost= 1.207984567 Accuray:  0.542594594224482\n",
      "Training step: 1900 cost= 1.167289376 Accuray:  0.548936841717284\n",
      "Training step: 1950 cost= 1.173462629 Accuray:  0.5551025636780721\n",
      "Training step: 2000 cost= 1.013154268 Accuray:  0.5610299995634704\n",
      "Training step: 2050 cost= 1.099529147 Accuray:  0.5668926825265332\n",
      "Training step: 2100 cost= 0.988134325 Accuray:  0.5722666661884813\n",
      "Training step: 2150 cost= 1.081840158 Accuray:  0.577479069213881\n",
      "Training step: 2200 cost= 1.083405137 Accuray:  0.5827545448951423\n",
      "Training step: 2250 cost= 1.014739394 Accuray:  0.5881288882444302\n",
      "Training step: 2300 cost= 0.930573702 Accuray:  0.5932391297120763\n",
      "Training step: 2350 cost= 1.047884822 Accuray:  0.5979872331799979\n",
      "Training step: 2400 cost= 0.876259327 Accuray:  0.6025208324519917\n",
      "Training step: 2450 cost= 0.861464977 Accuray:  0.6070979582123002\n",
      "Training step: 2500 cost= 0.892054677 Accuray:  0.6114279990270733\n",
      "Training step: 2550 cost= 0.843911529 Accuray:  0.6155803911665491\n",
      "Training step: 2600 cost= 0.774677992 Accuray:  0.6198769220552193\n",
      "Training step: 2650 cost= 0.845971286 Accuray:  0.623849055549165\n",
      "Training step: 2700 cost= 0.699528515 Accuray:  0.6278407397162583\n",
      "Training step: 2750 cost= 0.786459446 Accuray:  0.631719999016686\n",
      "Training step: 2800 cost= 0.689241111 Accuray:  0.6354607133208109\n",
      "Training step: 2850 cost= 0.772174537 Accuray:  0.6390736832391275\n",
      "Training step: 2900 cost= 0.774288476 Accuray:  0.6426586196815659\n",
      "Training step: 2950 cost= 0.711830735 Accuray:  0.6460508465021849\n",
      "Training step: 3000 cost= 0.548806906 Accuray:  0.6495233323437473\n",
      "Training step: 3050 cost= 0.559510291 Accuray:  0.6528360646144777\n",
      "Training step: 3100 cost= 0.766589344 Accuray:  0.6560129022850625\n",
      "Training step: 3150 cost= 0.758739412 Accuray:  0.6592126974853731\n",
      "Training step: 3200 cost= 0.612315297 Accuray:  0.6623499990871642\n",
      "Training step: 3250 cost= 0.637971997 Accuray:  0.665332306812589\n",
      "Training step: 3300 cost= 0.730193734 Accuray:  0.6682787869611021\n",
      "Training step: 3350 cost= 0.593617857 Accuray:  0.6711223871372084\n",
      "Training step: 3400 cost= 0.633076608 Accuray:  0.6739558814959052\n",
      "Training step: 3450 cost= 0.688599527 Accuray:  0.6767130426792562\n",
      "Training step: 3500 cost= 0.598760128 Accuray:  0.6793799991660885\n",
      "Training step: 3550 cost= 0.539076626 Accuray:  0.682005633003485\n",
      "Training step: 3600 cost= 0.551849365 Accuray:  0.6846194437032358\n",
      "Training step: 3650 cost= 0.579100251 Accuray:  0.6870986293817628\n",
      "Training step: 3700 cost= 0.561516464 Accuray:  0.6894972965896532\n",
      "Training step: 3750 cost= 0.571091712 Accuray:  0.6918453326592843\n",
      "Training step: 3800 cost= 0.428566903 Accuray:  0.694194736171906\n",
      "Training step: 3850 cost= 0.454676449 Accuray:  0.696599999330454\n",
      "Training step: 3900 cost= 0.560657322 Accuray:  0.6988974352725423\n",
      "Training step: 3950 cost= 0.582726777 Accuray:  0.7011518981383194\n",
      "Training step: 4000 cost= 0.398801148 Accuray:  0.703242499445565\n",
      "Training step: 4050 cost= 0.578749895 Accuray:  0.7053308636511182\n",
      "Training step: 4100 cost= 0.481487513 Accuray:  0.7073414628434835\n",
      "Training step: 4150 cost= 0.504873633 Accuray:  0.7094265054695936\n",
      "Training step: 4200 cost= 0.451113254 Accuray:  0.7114571423234329\n",
      "Training step: 4250 cost= 0.491687179 Accuray:  0.713390587712912\n",
      "Training step: 4300 cost= 0.574177086 Accuray:  0.7153116273646092\n",
      "Training step: 4350 cost= 0.454328120 Accuray:  0.7172298845246263\n",
      "Training step: 4400 cost= 0.379214257 Accuray:  0.7190840904186057\n",
      "Training step: 4450 cost= 0.658445716 Accuray:  0.7209617972683706\n",
      "Training step: 4500 cost= 0.326108009 Accuray:  0.72271555507597\n",
      "Training step: 4550 cost= 0.401942134 Accuray:  0.7245076918380928\n",
      "Training step: 4600 cost= 0.432875872 Accuray:  0.7262130429603807\n",
      "Training step: 4650 cost= 0.409034878 Accuray:  0.7279677414165069\n",
      "Training step: 4700 cost= 0.469121426 Accuray:  0.7295914888421589\n",
      "Training step: 4750 cost= 0.382677794 Accuray:  0.7311789468377828\n",
      "Training step: 4800 cost= 0.399610758 Accuray:  0.732789582812693\n",
      "Training step: 4850 cost= 0.552416921 Accuray:  0.734393813884289\n",
      "Training step: 4900 cost= 0.446200013 Accuray:  0.7359428565759136\n",
      "Training step: 4950 cost= 0.520720720 Accuray:  0.737395958988504\n",
      "Training step: 5000 cost= 0.431754082 Accuray:  0.7389039993919432\n",
      "Training step: 5050 cost= 0.498582393 Accuray:  0.7403841578053071\n",
      "Training step: 5100 cost= 0.469882935 Accuray:  0.7417882347143456\n",
      "Training step: 5150 cost= 0.400765002 Accuray:  0.7431631062367877\n",
      "Training step: 5200 cost= 0.453780174 Accuray:  0.7445634609955148\n",
      "Training step: 5250 cost= 0.377723515 Accuray:  0.7459485708567358\n",
      "Training step: 5300 cost= 0.366191715 Accuray:  0.7473754711343713\n",
      "Training step: 5350 cost= 0.415742368 Accuray:  0.7487121489483898\n",
      "Training step: 5400 cost= 0.447070509 Accuray:  0.750087036471814\n",
      "Training step: 5450 cost= 0.414908439 Accuray:  0.7513633021992554\n",
      "Training step: 5500 cost= 0.269592524 Accuray:  0.7526636358425021\n",
      "Training step: 5550 cost= 0.394559026 Accuray:  0.7539081076062746\n",
      "Training step: 5600 cost= 0.399916142 Accuray:  0.7551785709635753\n",
      "Training step: 5650 cost= 0.439770430 Accuray:  0.7563946898459597\n",
      "Training step: 5700 cost= 0.355341643 Accuray:  0.757631578532358\n",
      "Training step: 5750 cost= 0.384246051 Accuray:  0.7588156517724628\n",
      "Training step: 5800 cost= 0.281824678 Accuray:  0.7599879306381375\n",
      "Training step: 5850 cost= 0.302215010 Accuray:  0.76113162352807\n",
      "Training step: 5900 cost= 0.403122395 Accuray:  0.7622338979053548\n",
      "Training step: 5950 cost= 0.440023273 Accuray:  0.763337814720864\n",
      "Training step: 6000 cost= 0.302416354 Accuray:  0.7644733329347024\n",
      "Training step: 6050 cost= 0.399590075 Accuray:  0.765586776480325\n",
      "Training step: 6100 cost= 0.337845385 Accuray:  0.7666803275201408\n",
      "Training step: 6150 cost= 0.518998265 Accuray:  0.7676796744539728\n",
      "Training step: 6200 cost= 0.306073397 Accuray:  0.7687225803282232\n",
      "Training step: 6250 cost= 0.338953316 Accuray:  0.7697503996878863\n",
      "Training step: 6300 cost= 0.306212008 Accuray:  0.770792063184319\n",
      "Training step: 6350 cost= 0.278935581 Accuray:  0.7718346453587136\n",
      "Training step: 6400 cost= 0.297791600 Accuray:  0.7727812496881233\n",
      "Training step: 6450 cost= 0.243817523 Accuray:  0.7737705423232428\n",
      "Training step: 6500 cost= 0.434258819 Accuray:  0.7747753843281131\n",
      "Training step: 6550 cost= 0.343238235 Accuray:  0.77576183179237\n",
      "Training step: 6600 cost= 0.384633154 Accuray:  0.7766924239858759\n",
      "Training step: 6650 cost= 0.448523104 Accuray:  0.7776165410926692\n",
      "Training step: 6700 cost= 0.360050291 Accuray:  0.7785029848147906\n",
      "Training step: 6750 cost= 0.385820419 Accuray:  0.7793881478867045\n",
      "Training step: 6800 cost= 0.254214972 Accuray:  0.7802735291887075\n",
      "Training step: 6850 cost= 0.361401469 Accuray:  0.7812029194597998\n",
      "Training step: 6900 cost= 0.339711756 Accuray:  0.7820652171434915\n",
      "Training step: 6950 cost= 0.308448374 Accuray:  0.7829582731441842\n",
      "Training step: 7000 cost= 0.354238153 Accuray:  0.783822856931282\n",
      "Training step: 7050 cost= 0.341172367 Accuray:  0.7846695033735629\n",
      "Training step: 7100 cost= 0.307568938 Accuray:  0.785501408277368\n",
      "Training step: 7150 cost= 0.427065164 Accuray:  0.7862853145375327\n",
      "Training step: 7200 cost= 0.380238086 Accuray:  0.7870361109600506\n",
      "Training step: 7250 cost= 0.381477743 Accuray:  0.787877241219426\n",
      "Training step: 7300 cost= 0.272796869 Accuray:  0.7886698628722193\n",
      "Training step: 7350 cost= 0.334781796 Accuray:  0.7894816325579573\n",
      "Training step: 7400 cost= 0.359601200 Accuray:  0.790244594516887\n",
      "Training step: 7450 cost= 0.466716409 Accuray:  0.7910228187230809\n",
      "Training step: 7500 cost= 0.301272422 Accuray:  0.7917679999137918\n",
      "Training step: 7550 cost= 0.265858829 Accuray:  0.7925470197857019\n",
      "Training step: 7600 cost= 0.443299860 Accuray:  0.7933447367526395\n",
      "Training step: 7650 cost= 0.370464355 Accuray:  0.7940862744174947\n",
      "Training step: 7700 cost= 0.336985052 Accuray:  0.7947922077141218\n",
      "Training step: 7750 cost= 0.380549133 Accuray:  0.7954877418572864\n",
      "Training step: 7800 cost= 0.408800423 Accuray:  0.7962346152946926\n",
      "Training step: 7850 cost= 0.275777847 Accuray:  0.7969414012015436\n",
      "Training step: 7900 cost= 0.483232409 Accuray:  0.797617721484526\n",
      "Training step: 7950 cost= 0.286803007 Accuray:  0.7982754716956578\n",
      "Training step: 8000 cost= 0.385585070 Accuray:  0.7989075000085868\n",
      "Training step: 8050 cost= 0.300678194 Accuray:  0.7996062112025778\n",
      "Training step: 8100 cost= 0.260453105 Accuray:  0.8003111111552075\n",
      "Training step: 8150 cost= 0.287660033 Accuray:  0.8009570552696662\n",
      "Training step: 8200 cost= 0.269503176 Accuray:  0.80161707323726\n",
      "Training step: 8250 cost= 0.291212350 Accuray:  0.8022993940101428\n",
      "Training step: 8300 cost= 0.366104841 Accuray:  0.8029542169535914\n",
      "Training step: 8350 cost= 0.208353087 Accuray:  0.8036023953097488\n",
      "Training step: 8400 cost= 0.308039874 Accuray:  0.8042571429717577\n",
      "Training step: 8450 cost= 0.227518022 Accuray:  0.8048899409637825\n",
      "Training step: 8500 cost= 0.501960218 Accuray:  0.8054705883426702\n",
      "Training step: 8550 cost= 0.476500630 Accuray:  0.8060807018622494\n",
      "Training step: 8600 cost= 0.263863653 Accuray:  0.8066732558976253\n",
      "Training step: 8650 cost= 0.316870034 Accuray:  0.8072647399593124\n",
      "Training step: 8700 cost= 0.406984746 Accuray:  0.807849425374177\n",
      "Training step: 8750 cost= 0.275589377 Accuray:  0.8084868572392634\n",
      "Training step: 8800 cost= 0.253086776 Accuray:  0.8090795455420051\n",
      "Training step: 8850 cost= 0.359167337 Accuray:  0.8096350283360919\n",
      "Training step: 8900 cost= 0.333628774 Accuray:  0.8102089888481109\n",
      "Training step: 8950 cost= 0.389603227 Accuray:  0.810776536400175\n",
      "Training step: 9000 cost= 0.264408410 Accuray:  0.8113866667701967\n",
      "Training step: 9050 cost= 0.350997806 Accuray:  0.8119812155975986\n",
      "Training step: 9100 cost= 0.296741992 Accuray:  0.8125131869402069\n",
      "Training step: 9150 cost= 0.241458401 Accuray:  0.813001093032663\n",
      "Training step: 9200 cost= 0.296233475 Accuray:  0.8135500001603656\n",
      "Training step: 9250 cost= 0.267916381 Accuray:  0.8141156758702285\n",
      "Training step: 9300 cost= 0.372341603 Accuray:  0.8146645163420227\n",
      "Training step: 9350 cost= 0.264886290 Accuray:  0.8151807489070823\n",
      "Training step: 9400 cost= 0.234035969 Accuray:  0.8156765959891391\n",
      "Training step: 9450 cost= 0.291510075 Accuray:  0.8161968256639582\n",
      "Training step: 9500 cost= 0.264986128 Accuray:  0.8166800002569431\n",
      "Training step: 9550 cost= 0.201798394 Accuray:  0.8171947646659863\n",
      "Training step: 9600 cost= 0.346680999 Accuray:  0.8177135419492455\n",
      "Training step: 9650 cost= 0.267252207 Accuray:  0.8182466324210321\n",
      "Training step: 9700 cost= 0.272470981 Accuray:  0.8187546394615597\n",
      "Training step: 9750 cost= 0.234577253 Accuray:  0.8192451285112362\n",
      "Training step: 9800 cost= 0.160368636 Accuray:  0.8197438778579995\n",
      "Training step: 9850 cost= 0.270610839 Accuray:  0.8202680206317738\n",
      "Training step: 9900 cost= 0.306919485 Accuray:  0.8207171720706604\n",
      "Training step: 9950 cost= 0.326561213 Accuray:  0.8211819099140676\n",
      "0.9199\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "#paramètres\n",
    "learning_rate = 0.001\n",
    "training_epochs = 10000\n",
    "display_step = 50\n",
    "#n_samples = inputY.size\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "#Définition des neuronnes et des layers\n",
    "n_neurons_1 = 1024\n",
    "n_neurons_2 = 512\n",
    "n_neurons_3 = 256\n",
    "n_neurons_4 = 128\n",
    "\n",
    "#Definition des variables\n",
    "weight_initializer = tf.variance_scaling_initializer(mode=\"fan_avg\", distribution=\"uniform\", scale=1)\n",
    "bias_initializer = tf.zeros_initializer()\n",
    "\n",
    "W_hidden_1 = tf.Variable(weight_initializer([784, n_neurons_1]))\n",
    "bias_hidden_1 = tf.Variable(bias_initializer([n_neurons_1]))\n",
    "\n",
    "W_hidden_2 = tf.Variable(weight_initializer([n_neurons_1, n_neurons_2]))\n",
    "bias_hidden_2 = tf.Variable(bias_initializer([n_neurons_2]))\n",
    "\n",
    "W_hidden_3 = tf.Variable(weight_initializer([n_neurons_2, n_neurons_3]))\n",
    "bias_hidden_3 = tf.Variable(bias_initializer([n_neurons_3]))\n",
    "\n",
    "W_hidden_4 = tf.Variable(weight_initializer([n_neurons_3, n_neurons_4]))\n",
    "bias_hidden_4 = tf.Variable(bias_initializer([n_neurons_4]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "W_out = tf.Variable(weight_initializer([n_neurons_4, 10]))\n",
    "bias_out = tf.Variable(bias_initializer([10]))\n",
    "\n",
    "#Définition du graph\n",
    "\n",
    "hidden_1 = tf.nn.relu(tf.add(tf.matmul(X, W_hidden_1), bias_hidden_1))\n",
    "hidden_2 = tf.nn.relu(tf.add(tf.matmul(hidden_1, W_hidden_2), bias_hidden_2))\n",
    "hidden_3 = tf.nn.relu(tf.add(tf.matmul(hidden_2, W_hidden_3), bias_hidden_3))\n",
    "hidden_4 = tf.nn.relu(tf.add(tf.matmul(hidden_3, W_hidden_4), bias_hidden_4))\n",
    "\n",
    "out = tf.nn.softmax(tf.add(tf.matmul(hidden_4, W_out), bias_out))\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(out), reduction_indices=[1])) #cross entropy\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(out, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "sum_accuracy = 0\n",
    "for i in range(training_epochs):  \n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(opt, feed_dict={X: batch_xs, Y:batch_ys}) # Take a gradient descent step using our inputs and labels\n",
    "    sum_accuracy += sess.run(accuracy, feed_dict={X:batch_xs, Y:batch_ys})\n",
    "    # That's all! The rest of the cell just outputs debug messages. \n",
    "    # Display logs per epoch step\n",
    "    if (i) % display_step == 0:\n",
    "        #summary_str = cost_summary.eval(feed_dict={X: inputX, Y:inputY}, session= sess)\n",
    "        cc = sess.run(cost, feed_dict={X: batch_xs, Y:batch_ys})\n",
    "        #file_writer.add_summary(summary_str, i)\n",
    "        avg_accuracy = sum_accuracy / i\n",
    "        print(\"Training step:\", '%04d' % (i), \"cost=\", \"{:.9f}\".format(cc), \"Accuracy: \", avg_accuracy) #, \\\"W=\", sess.run(W), \"b=\", sess.run(b)\n",
    "#file_writer.close()\n",
    "\n",
    "print(sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
    "                                     Y: mnist.test.labels}))\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "training_cost = sess.run(cost, feed_dict={X: batch_xs, Y: batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
